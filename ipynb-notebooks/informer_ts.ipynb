{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5e92e4",
   "metadata": {
    "id": "ff5e92e4"
   },
   "source": [
    "# Multivariate Probabilistic Time Series Forecasting with Informer\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A few months ago we introduced the [Time Series Transformer](https://huggingface.co/blog/time-series-transformers), which is the vanilla Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)) applied to forecasting, and showed an example for the **univariate** probabilistic forecasting task (i.e. predicting each time series' 1-d distribution individually). In this post we introduce the _Informer_ model ([Zhou, Haoyi, et al., 2021](https://arxiv.org/abs/2012.07436)), AAAI21 best paper which is [now available](https://huggingface.co/docs/transformers/main/en/model_doc/informer) in ðŸ¤— Transformers. We will show how to use the Informer model for the **multivariate** probabilistic forecasting task, i.e., predicting the distribution of a future **vector** of time-series target values. Note that this will also work for the vanilla Time Series Transformer model.\n",
    "\n",
    "##  Multivariate Probabilistic Time Series Forecasting\n",
    "\n",
    "As far as the modeling aspect of probabilistic forecasting is concerned, the Transformer/Informer will require no change when dealing with multivariate time series. In both the univariate and multivariate setting, the model will receive a sequence of vectors and thus the only change is on the output or emission side.\n",
    "\n",
    "Modeling the full joint conditional distribution of high dimensional data can get computationally expensive and thus methods resort to some approximation of the distribution, the easiest being to model the data as an independent distribution from the same family, or some low-rank approximation to the full covariance, etc. Here we will just resort to the independent (or diagonal) emissions which are supported for the families of distributions we have implemented [here](https://huggingface.co/docs/transformers/main/en/internal/time_series_utils).\n",
    "\n",
    "## Informer - Under The Hood\n",
    "\n",
    "Based on the vanilla Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), Informer employs two major improvements. To understand these improvements, let's recall the drawbacks of the vanilla Transformer:\n",
    "\n",
    "1. **Quadratic computation of canonical self-attention:** The vanilla Transformer has a computational complexity of $O(T^2 D)$ where $T$ is the time series length and $D$ is the dimension of the hidden states. For long sequence time-series forecasting (also known as the _LSTF problem_), this might be really computationally expensive. To solve this problem, Informer employs a new self-attention mechanism called _ProbSparse_ attention, which has $O(T \\log T)$ time and space complexity.\n",
    "1. **Memory bottleneck when stacking layers:** When stacking $N$ encoder/decoder layers, the vanilla Transformer has a memory usage of $O(N T^2)$, which limits the model's capacity for long sequences. Informer uses a _Distilling_ operation, for reducing the input size between layers into its half slice. By doing so, it reduces the whole memory usage to be $O(N\\cdot T \\log T)$.\n",
    "\n",
    "As you can see, the motivation for the Informer model is similar to Longformer ([Beltagy et el., 2020](https://arxiv.org/abs/2004.05150)), Sparse Transformer ([Child et al., 2019](https://arxiv.org/abs/1904.10509)) and other NLP papers for reducing the quadratic complexity of the self-attention mechanism **when the input sequence is long**. Now, let's dive into _ProbSparse_ attention and the _Distilling_ operation with code examples.\n",
    "\n",
    "### ProbSparse Attention\n",
    "\n",
    "The main idea of ProbSparse is that the canonical self-attention scores form a long-tail distribution, where the \"active\" queries lie in the \"head\" scores and \"lazy\" queries lie in the \"tail\" area. By \"active\" query we mean a query $q_i$ such that the dot-product $\\langle q_i,k_i \\rangle$ **contributes** to the major attention, whereas a \"lazy\" query forms a dot-product which generates  **trivial** attention. Here, $q_i$ and $k_i$ are the $i$-th rows in $Q$ and $K$ attention matrices respectively.\n",
    "\n",
    "| ![informer_full_vs_sparse_attention](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/informer_full_vs_sparse_attention.png) |\n",
    "|:--:|\n",
    "|  Vanilla self attention vs ProbSparse attention from [Autoformer (Wu, Haixu, et al., 2021)](https://wuhaixu2016.github.io/pdf/NeurIPS2021_Autoformer.pdf) |\n",
    "\n",
    "Given the idea of \"active\" and \"lazy\" queries, the ProbSparse attention selects the \"active\" queries, and creates a reduced query matrix $Q_{reduced}$ which is used to calculate the attention weights in $O(T \\log T)$. Let's see this more in detail with a code example.\n",
    "    \n",
    "Recall the canonical self-attention formula:\n",
    "\n",
    "$$\n",
    "\\textrm{Attention}(Q, K, V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}} )V\n",
    "$$\n",
    "\n",
    "Where $Q\\in \\mathbb{R}^{L_Q \\times d}, K\\in \\mathbb{R}^{L_K \\times d}, V\\in \\mathbb{R}^{L_V \\times d}$. Note that in practice, the input length of queries and keys are typically equivalent in the self-attention computation, i.e. $L_Q = L_K = T$ where $T$ is the time series length. Therefore, the $QK^T$ multiplication takes $O(T^2 \\cdot d)$ computational complexity. In ProbSparse attention, our goal is to create a new $Q_{reduce}$ matrix and define:\n",
    "\n",
    "$$\n",
    "\\textrm{ProbSparseAttention}(Q, K, V) = \\textrm{softmax}(\\frac{Q_{reduce}K^T}{\\sqrt{d_k}} )V\n",
    "$$\n",
    "\n",
    "where the $Q_{reduce}$ matrix only selects the Top $u$ \"active\" queries. Here, $u = c \\cdot \\log L_Q$ and $c$ called the _sampling factor_ hyperparameter for the ProbSparse attention. Since $Q_{reduce}$ selects only the Top $u$ queries, its size is $c\\cdot \\log L_Q \\times d$, so the multiplication $Q_{reduce}K^T$ takes only $O(L_K \\log L_Q) = O(T \\log T)$.\n",
    "\n",
    "This is good! But how can we select the $u$ \"active\" queries to create $Q_{reduce}$? Let's define the _Query Sparsity Measurement_.\n",
    "\n",
    "#### Query Sparsity Measurement\n",
    "Query Sparsity Measurement $M(q_i, K)$ is used for selecting the $u$ \"active\" queries $q_i$ in $Q$ to create $Q_{reduce}$. In theory, the dominant $\\langle q_i,k_i \\rangle$ pairs encourage the \"active\" $q_i$'s probability distribution **away** from the uniform distribution as can be seen in the figure below. Hence, the [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the actual queries distribution and the uniform distribution is used to define the sparsity measurement.\n",
    "\n",
    "| ![informer_probsparse](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/informer_probsparse.png) |\n",
    "|:--:|\n",
    "| The illustration of ProbSparse Attention from official [repository](https://github.com/zhouhaoyi/Informer2020)|\n",
    "\n",
    "\n",
    "In practice, the measurement is defined as:\n",
    "\n",
    "$$\n",
    "M(q_i, K) = \\max_j \\frac{q_ik_j^T}{\\sqrt{d}}-\\frac{1}{L_k} \\sum_{j=1}^{L_k}\\frac{q_ik_j^T}{\\sqrt{d}}\n",
    "$$\n",
    "\n",
    "\n",
    "The important thing to understand here is when $M(q_i, K)$ is larger, the query $q_i$ should be in $Q_{reduce}$ and vice versa.\n",
    "\n",
    "But how can we calculate the term $q_ik_j^T$ in non-quadratic time? Recall that most of the dot-product $\\langle q_i,k_i \\rangle$ generate either way the trivial attention (i.e. long-tail distribution property), so it is enough to randomly sample a subset of keys from $K$, which will be called `K_sample` in the code.\n",
    "\n",
    "Now, we are ready to see the code of `probsparse_attention`:\n",
    "    \n",
    "```python\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "def probsparse_attention(query_states, key_states, value_states, sampling_factor=5):\n",
    "    \"\"\"\n",
    "    Compute the probsparse self-attention.\n",
    "    Input shape: Batch x Time x Channel\n",
    "\n",
    "    Note the additional `sampling_factor` input.\n",
    "    \"\"\"\n",
    "    # get input sizes with logs\n",
    "    L_K = key_states.size(1)\n",
    "    L_Q = query_states.size(1)\n",
    "    log_L_K = np.ceil(np.log1p(L_K)).astype(\"int\").item()\n",
    "    log_L_Q = np.ceil(np.log1p(L_Q)).astype(\"int\").item()\n",
    "\n",
    "    # calculate a subset of samples to slice from K and create Q_K_sample\n",
    "    U_part = min(sampling_factor * L_Q * log_L_K, L_K)\n",
    "\n",
    "    # create Q_K_sample (the q_i * k_j^T term in the sparsity measurement)\n",
    "    index_sample = torch.randint(0, L_K, (U_part,))\n",
    "    K_sample = key_states[:, index_sample, :]\n",
    "    Q_K_sample = torch.bmm(query_states, K_sample.transpose(1, 2))\n",
    "\n",
    "    # calculate the query sparsity measurement with Q_K_sample\n",
    "    M = Q_K_sample.max(dim=-1)[0] - torch.div(Q_K_sample.sum(dim=-1), L_K)\n",
    "\n",
    "    # calculate u to find the Top-u queries under the sparsity measurement\n",
    "    u = min(sampling_factor * log_L_Q, L_Q)\n",
    "    M_top = M.topk(u, sorted=False)[1]\n",
    "\n",
    "    # calculate Q_reduce as query_states[:, M_top]\n",
    "    dim_for_slice = torch.arange(query_states.size(0)).unsqueeze(-1)\n",
    "    Q_reduce = query_states[dim_for_slice, M_top]  # size: c*log_L_Q x channel\n",
    "\n",
    "    # and now, same as the canonical\n",
    "    d_k = query_states.size(-1)\n",
    "    attn_scores = torch.bmm(Q_reduce, key_states.transpose(-2, -1))  # Q_reduce x K^T\n",
    "    attn_scores = attn_scores / math.sqrt(d_k)\n",
    "    attn_probs = nn.functional.softmax(attn_scores, dim=-1)\n",
    "    attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "    return attn_output, attn_scores\n",
    "```\n",
    "Note that in the implementation, $U_{part}$ contain $L_Q$ in the calculation for stability issues (see [this disccusion](https://discuss.huggingface.co/t/probsparse-attention-in-informer/34428) for more information).\n",
    "\n",
    "We did it! Please be aware that this is only a partial implementation of the `probsparse_attention`, and the full implementation can be found in ðŸ¤— Transformers.\n",
    "\n",
    "### Distilling\n",
    "\n",
    "Because of the ProbSparse self-attention, the encoderâ€™s feature map has some redundancy that can be removed. Therefore,\n",
    "the distilling operation is used to reduce the input size between encoder layers into its half slice, thus in theory removing this redundancy. In practice, Informer's \"distilling\" operation just adds 1D convolution layers with max pooling between each of the encoder layers. Let $X_n$ be the output of the $n$-th encoder layer, the distilling operation is then defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "X_{n+1} = \\textrm{MaxPool} ( \\textrm{ELU}(\\textrm{Conv1d}(X_n))\n",
    "$$\n",
    "\n",
    "\n",
    "Let's see this in code:\n",
    "    \n",
    "```python\n",
    "from torch import nn\n",
    "\n",
    "# ConvLayer is a class with forward pass applying ELU and MaxPool1d\n",
    "def informer_encoder_forward(x_input, num_encoder_layers=3, distil=True):\n",
    "    # Initialize the convolution layers\n",
    "    if distil:\n",
    "        conv_layers = nn.ModuleList([ConvLayer() for _ in range(num_encoder_layers - 1)])\n",
    "        conv_layers.append(None)\n",
    "    else:\n",
    "        conv_layers = [None] * num_encoder_layers\n",
    "    \n",
    "    # Apply conv_layer between each encoder_layer\n",
    "    for encoder_layer, conv_layer in zip(encoder_layers, conv_layers):\n",
    "        output = encoder_layer(x_input)\n",
    "        if conv_layer is not None:\n",
    "            output = conv_layer(loutput)\n",
    "    \n",
    "    return output\n",
    "```\n",
    "    \n",
    "By reducing the input of each layer by two, we get a memory usage of $O(N\\cdot T \\log T)$ instead of $O(N\\cdot T^2)$ where $N$ is the number of encoder/decoder layers. This is what we wanted!\n",
    "    \n",
    "The Informer model in [now available](https://huggingface.co/docs/transformers/main/en/model_doc/informer) in the ðŸ¤— Transformers library, and simply called `InformerModel`. In the sections below, we will show how to train this model on a custom multivariate time-series dataset.\n",
    "\n",
    "\n",
    "## Set-up Environment\n",
    "\n",
    "First, let's install the necessary libraries: ðŸ¤— Transformers, ðŸ¤— Datasets, ðŸ¤— Evaluate,  ðŸ¤— Accelerate and [GluonTS](https://github.com/awslabs/gluonts).\n",
    "\n",
    "As we will show, GluonTS will be used for transforming the data to create features as well as for creating appropriate training, validation and test batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2ddc5-00e8-47a9-9d5a-855b5853a479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7AkJMJAunLP9",
   "metadata": {
    "id": "7AkJMJAunLP9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets evaluate accelerate gluonts ujson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaebea67",
   "metadata": {
    "id": "eaebea67"
   },
   "source": [
    "We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c2cf63",
   "metadata": {
    "id": "98c2cf63"
   },
   "outputs": [],
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"multivariate_informer_notebook\", framework=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4m_7_MKqmzfE",
   "metadata": {
    "id": "4m_7_MKqmzfE"
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "In this blog post, we'll use the `traffic_hourly` dataset, which is available on the [Hugging Face Hub](https://huggingface.co/datasets/monash_tsf). This dataset contains the San Francisco Traffic dataset used by [Lai et al. (2017)](https://arxiv.org/abs/1703.07015). It contains 862 hourly time series showing the road occupancy rates in the range $[0, 1]$ on the San Francisco Bay area freeways from 2015 to 2016.\n",
    "\n",
    "This dataset is part of the [Monash Time Series Forecasting](https://forecastingdata.org/) repository, a collection of time series datasets from a number of domains. It can be viewed as the [GLUE benchmark](https://gluebenchmark.com/) of time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d93a3f41",
   "metadata": {
    "id": "d93a3f41"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"monash_tsf\", \"traffic_weekly\")\n",
    "tc_dataset = load_dataset(\"shaddie/thrust_curves_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sG1ZNNzwpwmg",
   "metadata": {
    "id": "sG1ZNNzwpwmg"
   },
   "source": [
    "As can be seen, the dataset contains 3 splits: train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c7eafe57-23c7-437d-949e-4cbe8031383d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'],\n",
       "        num_rows: 266\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d45ae147",
   "metadata": {
    "id": "d45ae147"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'],\n",
       "        num_rows: 266\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "132cb4da-b688-4dcd-bd28-8e272eab71cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0  6\n",
      "index: 1  14\n",
      "index: 2  37\n",
      "index: 3  15\n",
      "index: 4  12\n",
      "index: 5  23\n",
      "index: 6  9\n",
      "index: 7  31\n",
      "index: 8  19\n",
      "index: 9  25\n",
      "index: 10  4\n",
      "index: 11  13\n",
      "index: 12  19\n",
      "index: 13  26\n",
      "index: 14  10\n",
      "index: 15  25\n",
      "index: 16  12\n",
      "index: 17  20\n",
      "index: 18  22\n",
      "index: 19  27\n",
      "index: 20  18\n",
      "index: 21  20\n",
      "index: 22  26\n",
      "index: 23  23\n",
      "index: 24  23\n",
      "index: 25  27\n",
      "index: 26  13\n",
      "index: 27  18\n",
      "index: 28  21\n",
      "index: 29  29\n",
      "index: 30  32\n",
      "index: 31  23\n",
      "index: 32  8\n",
      "index: 33  6\n",
      "index: 34  61\n",
      "index: 35  7\n",
      "index: 36  13\n",
      "index: 37  24\n",
      "index: 38  22\n",
      "index: 39  26\n",
      "index: 40  23\n",
      "index: 41  9\n",
      "index: 42  11\n",
      "index: 43  25\n",
      "index: 44  14\n",
      "index: 45  6\n",
      "index: 46  23\n",
      "index: 47  13\n",
      "index: 48  8\n",
      "index: 49  15\n",
      "index: 50  13\n",
      "index: 51  13\n",
      "index: 52  25\n",
      "index: 53  27\n",
      "index: 54  5\n",
      "index: 55  31\n",
      "index: 56  27\n",
      "index: 57  30\n",
      "index: 58  9\n",
      "index: 59  29\n",
      "index: 60  17\n",
      "index: 61  31\n",
      "index: 62  18\n",
      "index: 63  29\n",
      "index: 64  24\n",
      "index: 65  11\n",
      "index: 66  49\n",
      "index: 67  31\n",
      "index: 68  16\n",
      "index: 69  11\n",
      "index: 70  12\n",
      "index: 71  31\n",
      "index: 72  6\n",
      "index: 73  4\n",
      "index: 74  30\n",
      "index: 75  30\n",
      "index: 76  18\n",
      "index: 77  30\n",
      "index: 78  30\n",
      "index: 79  22\n",
      "index: 80  26\n",
      "index: 81  27\n",
      "index: 82  20\n",
      "index: 83  30\n",
      "index: 84  21\n",
      "index: 85  25\n",
      "index: 86  28\n",
      "index: 87  5\n",
      "index: 88  8\n",
      "index: 89  26\n",
      "index: 90  24\n",
      "index: 91  47\n",
      "index: 92  24\n",
      "index: 93  8\n",
      "index: 94  14\n",
      "index: 95  39\n",
      "index: 96  29\n",
      "index: 97  29\n",
      "index: 98  10\n",
      "index: 99  34\n",
      "index: 100  39\n",
      "index: 101  37\n",
      "index: 102  10\n",
      "index: 103  16\n",
      "index: 104  32\n",
      "index: 105  38\n",
      "index: 106  24\n",
      "index: 107  21\n",
      "index: 108  11\n",
      "index: 109  26\n",
      "index: 110  8\n",
      "index: 111  32\n",
      "index: 112  16\n",
      "index: 113  28\n",
      "index: 114  19\n",
      "index: 115  28\n",
      "index: 116  21\n",
      "index: 117  22\n",
      "index: 118  26\n",
      "index: 119  17\n",
      "index: 120  24\n",
      "index: 121  25\n",
      "index: 122  15\n",
      "index: 123  28\n",
      "index: 124  18\n",
      "index: 125  39\n",
      "index: 126  16\n",
      "index: 127  36\n",
      "index: 128  26\n",
      "index: 129  30\n",
      "index: 130  24\n",
      "index: 131  22\n",
      "index: 132  43\n",
      "index: 133  24\n",
      "index: 134  11\n",
      "index: 135  15\n",
      "index: 136  11\n",
      "index: 137  28\n",
      "index: 138  29\n",
      "index: 139  6\n",
      "index: 140  24\n",
      "index: 141  10\n",
      "index: 142  18\n",
      "index: 143  43\n",
      "index: 144  31\n",
      "index: 145  16\n",
      "index: 146  45\n",
      "index: 147  6\n",
      "index: 148  13\n",
      "index: 149  16\n",
      "index: 150  7\n",
      "index: 151  7\n",
      "index: 152  13\n",
      "index: 153  19\n",
      "index: 154  14\n",
      "index: 155  30\n",
      "index: 156  17\n",
      "index: 157  27\n",
      "index: 158  20\n",
      "index: 159  10\n",
      "index: 160  28\n",
      "index: 161  23\n",
      "index: 162  24\n",
      "index: 163  30\n",
      "index: 164  19\n",
      "index: 165  54\n",
      "index: 166  32\n",
      "index: 167  15\n",
      "index: 168  30\n",
      "index: 169  21\n",
      "index: 170  18\n",
      "index: 171  23\n",
      "index: 172  53\n",
      "index: 173  26\n",
      "index: 174  29\n",
      "index: 175  14\n",
      "index: 176  23\n",
      "index: 177  18\n",
      "index: 178  11\n",
      "index: 179  7\n",
      "index: 180  26\n",
      "index: 181  10\n",
      "index: 182  22\n",
      "index: 183  13\n",
      "index: 184  18\n",
      "index: 185  14\n",
      "index: 186  26\n",
      "index: 187  31\n",
      "index: 188  19\n",
      "index: 189  30\n",
      "index: 190  23\n",
      "index: 191  30\n",
      "index: 192  21\n",
      "index: 193  24\n",
      "index: 194  18\n",
      "index: 195  19\n",
      "index: 196  16\n",
      "index: 197  31\n",
      "index: 198  26\n",
      "index: 199  30\n",
      "index: 200  16\n",
      "index: 201  22\n",
      "index: 202  7\n",
      "index: 203  23\n",
      "index: 204  30\n",
      "index: 205  13\n",
      "index: 206  30\n",
      "index: 207  12\n",
      "index: 208  33\n",
      "index: 209  6\n",
      "index: 210  6\n",
      "index: 211  3\n",
      "index: 212  11\n",
      "index: 213  11\n",
      "index: 214  26\n",
      "index: 215  8\n",
      "index: 216  11\n",
      "index: 217  9\n",
      "index: 218  17\n",
      "index: 219  19\n",
      "index: 220  15\n",
      "index: 221  6\n",
      "index: 222  25\n",
      "index: 223  11\n",
      "index: 224  24\n",
      "index: 225  11\n",
      "index: 226  11\n",
      "index: 227  26\n",
      "index: 228  13\n",
      "index: 229  17\n",
      "index: 230  11\n",
      "index: 231  31\n",
      "index: 232  12\n",
      "index: 233  31\n",
      "index: 234  25\n",
      "index: 235  8\n",
      "index: 236  29\n",
      "index: 237  29\n",
      "index: 238  24\n",
      "index: 239  32\n",
      "index: 240  31\n",
      "index: 241  31\n",
      "index: 242  21\n",
      "index: 243  7\n",
      "index: 244  49\n",
      "index: 245  18\n",
      "index: 246  27\n",
      "index: 247  23\n",
      "index: 248  46\n",
      "index: 249  10\n",
      "index: 250  30\n",
      "index: 251  36\n",
      "index: 252  25\n",
      "index: 253  28\n",
      "index: 254  30\n",
      "index: 255  23\n",
      "index: 256  11\n",
      "index: 257  9\n",
      "index: 258  8\n",
      "index: 259  22\n",
      "index: 260  12\n",
      "index: 261  16\n",
      "index: 262  41\n",
      "index: 263  39\n",
      "index: 264  9\n",
      "index: 265  29\n",
      "max: 61\n"
     ]
    }
   ],
   "source": [
    "l=len(tc_dataset['train'])\n",
    "m = 0\n",
    "for r in range(l):\n",
    "    real = tc_dataset['train'][r]['feat_dynamic_real']\n",
    "    target = tc_dataset['train'][r]['target']\n",
    "    if m < len(target):\n",
    "        m = len(target)\n",
    "    print(f'index: {r}  {len(target)}')\n",
    "\n",
    "print(f'max: {m}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6886ae15-3e8f-4720-952e-9e70348f8df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=len(tc_dataset['train'])\n",
    "tcf = []\n",
    "m = 61\n",
    "for r in range(l):\n",
    "    real = tc_dataset['train'][r]['feat_dynamic_real']\n",
    "    target = tc_dataset['train'][r]['target']\n",
    "    rlen = len(real)\n",
    "    pad = [0 for _ in range(61-rlen)]\n",
    "    # target = tc_dataset['train'][r]['target']\n",
    "    real = real + pad\n",
    "    target = target + pad\n",
    "    # tc_dataset['train'][r]['feat_dynamic_real'] = real\n",
    "    # tc_dataset['train'][r]['target'] = target\n",
    "    tcf.append({\n",
    "        'start': tc_dataset['train'][r]['start'],\n",
    "        'target': target,\n",
    "        'feat_static_cat': tc_dataset['train'][r]['feat_static_cat'],\n",
    "        'feat_dynamic_real': real,\n",
    "        'item_id': tc_dataset['train'][r]['item_id']\n",
    "            }\n",
    "            )\n",
    "    # if m < len(target):\n",
    "    #     m = len(target)\n",
    "    # print(f'index: {r}  {len(target)}')\n",
    "tc = {\"train\": tcf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "22ba8f0f-ef20-4c51-ad86-7ee0b490f44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'],\n",
       "    num_rows: 266\n",
       "})"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "tc_hf_dataset = Dataset.from_list(tcf)\n",
    "tc_hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8f8c2fe7-8df0-4cd2-969c-975ce04ccf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'],\n",
       "    num_rows: 266\n",
       "})"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=tc_hf_dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26KIJLY2qFXI",
   "metadata": {
    "id": "26KIJLY2qFXI"
   },
   "source": [
    "Each example contains a few keys, of which `start` and `target` are the most important ones. Let us have a look at the first time series in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "oHlRCUPkoN1N",
   "metadata": {
    "id": "oHlRCUPkoN1N"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_example = dataset[0]\n",
    "train_example.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gbVF5vAcqzJG",
   "metadata": {
    "id": "gbVF5vAcqzJG"
   },
   "source": [
    "The `start` simply indicates the start of the time series (as a datetime), and the `target` contains the actual values of the time series.\n",
    "\n",
    "The `start` will be useful to add time related features to the time series values, as extra input to the model (such as \"month of year\"). Since we know the frequency of the data is `hourly`, we know for instance that the second value has the timestamp `2015-01-01 01:00:01`, `2015-01-01 02:00:01`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1PDt8bvwoUbN",
   "metadata": {
    "id": "1PDt8bvwoUbN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01 00:00:00\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "print(train_example[\"start\"])\n",
    "print(len(train_example[\"target\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DfkPxhCkquKL",
   "metadata": {
    "id": "DfkPxhCkquKL"
   },
   "source": [
    "The validation set contains the same data as the training set, just for a `prediction_length` longer amount of time. This allows us to validate the model's predictions against the ground truth.\n",
    "\n",
    "The test set is again one `prediction_length` longer data compared to the validation set (or some multiple of  `prediction_length` longer data compared to the training set for testing on multiple rolling windows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "eRQhm4EGpa0y",
   "metadata": {
    "id": "eRQhm4EGpa0y"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column validation not in the dataset. Current columns in the dataset: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[169]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m validation_example = \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalidation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m      2\u001b[39m validation_example.keys()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/datasets/arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/datasets/arrow_dataset.py:2761\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2759\u001b[39m format_kwargs = format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2761\u001b[39m pa_subtable = \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2762\u001b[39m formatted_output = format_table(\n\u001b[32m   2763\u001b[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001b[32m   2764\u001b[39m )\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/datasets/formatting/formatting.py:604\u001b[39m, in \u001b[36mquery_table\u001b[39m\u001b[34m(table, key, indices)\u001b[39m\n\u001b[32m    602\u001b[39m         _raise_bad_key_type(key)\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    606\u001b[39m     size = indices.num_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table.num_rows\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/datasets/formatting/formatting.py:541\u001b[39m, in \u001b[36m_check_valid_column_key\u001b[39m\u001b[34m(key, columns)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    540\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"Column validation not in the dataset. Current columns in the dataset: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id']\""
     ]
    }
   ],
   "source": [
    "validation_example = dataset[\"validation\"][0]\n",
    "validation_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "34e1dab4-5cb1-4191-92dd-d86ea22d9086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_example[\"target\"])\n",
    "# len(train_example[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x5PI_Jn7rDfj",
   "metadata": {
    "id": "x5PI_Jn7rDfj"
   },
   "source": [
    "The initial values are exactly the same as the corresponding training example. However, this example has `prediction_length=48` (48 hours, or 2 days) additional values compared to the training example. Let us verify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "__j4Z5Ohp8gg",
   "metadata": {
    "id": "__j4Z5Ohp8gg"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column validation not in the dataset. Current columns in the dataset: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[171]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m freq = \u001b[33m\"\u001b[39m\u001b[33m1W\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m prediction_length = \u001b[32m8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_example[\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m]) + prediction_length == \u001b[38;5;28mlen\u001b[39m(\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalidation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/datasets/arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/datasets/arrow_dataset.py:2761\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2759\u001b[39m format_kwargs = format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2761\u001b[39m pa_subtable = \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2762\u001b[39m formatted_output = format_table(\n\u001b[32m   2763\u001b[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001b[32m   2764\u001b[39m )\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/datasets/formatting/formatting.py:604\u001b[39m, in \u001b[36mquery_table\u001b[39m\u001b[34m(table, key, indices)\u001b[39m\n\u001b[32m    602\u001b[39m         _raise_bad_key_type(key)\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    606\u001b[39m     size = indices.num_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table.num_rows\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/datasets/formatting/formatting.py:541\u001b[39m, in \u001b[36m_check_valid_column_key\u001b[39m\u001b[34m(key, columns)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    540\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"Column validation not in the dataset. Current columns in the dataset: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id']\""
     ]
    }
   ],
   "source": [
    "freq = \"1W\"\n",
    "prediction_length = 8\n",
    "\n",
    "assert len(train_example[\"target\"]) + prediction_length == len(\n",
    "    dataset[\"validation\"][0][\"target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PGq2e9D-rhtg",
   "metadata": {
    "id": "PGq2e9D-rhtg"
   },
   "source": [
    "Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "cYDyml0tsnlL",
   "metadata": {
    "id": "cYDyml0tsnlL"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_of_samples = 150\n",
    "\n",
    "figure, axes = plt.subplots()\n",
    "axes.plot(train_example[\"target\"][-num_of_samples:], color=\"blue\")\n",
    "# axes.plot(\n",
    "#     validation_example[\"target\"][-num_of_samples - prediction_length :],\n",
    "#     color=\"red\",\n",
    "#     alpha=0.5,\n",
    "# )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4pGV6_ZduUaA",
   "metadata": {
    "id": "4pGV6_ZduUaA"
   },
   "source": [
    "Let's split up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "273c8b98-67bc-4198-9753-85575ea92005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'],\n",
       "    num_rows: 266\n",
       "})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1e29912c-f066-45f1-97fe-8e041bfd3591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]['item_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7eb15a6a",
   "metadata": {
    "id": "7eb15a6a"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset\n",
    "# test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5618d2e5-88e4-4114-8091-617239a2bf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset[\"start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "25930c79-ba1a-4f13-bbb1-aad2ea78228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in train_dataset:\n",
    "#     print(f'k, v in train_dataset {len(k)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125722c",
   "metadata": {
    "id": "0125722c"
   },
   "source": [
    "## Update `start` to `pd.Period`\n",
    "\n",
    "The first thing we'll do is convert the `start` feature of each time series to a pandas `Period` index using the data's `freq`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "249a9da4",
   "metadata": {
    "id": "249a9da4"
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@lru_cache(10_000)\n",
    "def convert_to_pandas_period(date, freq):\n",
    "    return pd.Period(date, freq)\n",
    "\n",
    "\n",
    "def transform_start_field(batch, freq):\n",
    "    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D7goNkUB1MPB",
   "metadata": {
    "id": "D7goNkUB1MPB"
   },
   "source": [
    "We now use `datasets`' [`set_transform`](https://huggingface.co/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.set_transform) functionality to do this on-the-fly in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b43c7551",
   "metadata": {
    "id": "b43c7551"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "train_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
    "test_dataset.set_transform(partial(transform_start_field, freq=freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909db77b",
   "metadata": {
    "id": "909db77b"
   },
   "source": [
    "Now, let's convert the dataset into a multivariate time series using the `MultivariateGrouper` from GluonTS. This grouper will convert the individual 1-dimensional time series into a single 2D matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0088e817",
   "metadata": {
    "id": "0088e817"
   },
   "outputs": [],
   "source": [
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "\n",
    "num_of_variates = len(train_dataset)\n",
    "\n",
    "train_grouper = MultivariateGrouper(max_target_dim=num_of_variates)\n",
    "# test_grouper = MultivariateGrouper(\n",
    "#     max_target_dim=num_of_variates,\n",
    "#     num_test_dates= None, # len(test_dataset)\n",
    "#     num_of_variates  # number of rolling test windows\n",
    "# )\n",
    "\n",
    "multi_variate_train_dataset = train_grouper(train_dataset)\n",
    "# multi_variate_test_dataset = test_grouper(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469f5ec3",
   "metadata": {
    "id": "469f5ec3"
   },
   "source": [
    "Note that now the target is 2 dimensional, where the first dim is the number of variates (number of time-series) and the second is the time-series values (time dimension):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "12a8e9c1",
   "metadata": {
    "id": "12a8e9c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_variate_train_example['target'].shape = (266, 99)\n"
     ]
    }
   ],
   "source": [
    "multi_variate_train_example = multi_variate_train_dataset[0]\n",
    "print(\n",
    "    f\"multi_variate_train_example['target'].shape = {multi_variate_train_example['target'].shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efb119",
   "metadata": {
    "id": "50efb119"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "Next, let's instantiate a model. The model will be trained from scratch, hence we won't use the `from_pretrained` method here, but rather randomly initialize the model from a [`config`](https://huggingface.co/docs/transformers/main/en/model_doc/informer#transformers.InformerConfig).\n",
    "\n",
    "We specify a couple of additional parameters to the model:\n",
    "- `prediction_length` (in our case, `48` hours): this is the horizon that the decoder of the Informer will learn to predict for;\n",
    "- `context_length`: the model will set the `context_length` (input of the encoder) equal to the `prediction_length`, if no `context_length` is specified;\n",
    "- `lags` for a given frequency: these specify an efficient \"look back\" mechanism, where we concatenate values from the past to the current values as additional features, e.g. for a `Daily` frequency we might consider a look back of `[1, 7, 30, ...]` or for `Minute` data we might consider `[1, 30, 60, 60*24, ...]` etc.;\n",
    "- the number of time features: in our case, this will be `5` as we'll add `HourOfDay`, `DayOfWeek`, ..., and `Age` features (see below).\n",
    "\n",
    "Let us check the default lags provided by GluonTS for the given frequency (\"hourly\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6CF4M8Ms7W-q",
   "metadata": {
    "id": "6CF4M8Ms7W-q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 12, 51, 52, 53, 103, 104, 105, 155, 156, 157]\n"
     ]
    }
   ],
   "source": [
    "from gluonts.time_feature import get_lags_for_frequency\n",
    "\n",
    "lags_sequence = get_lags_for_frequency(freq)\n",
    "print(lags_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q6ZuFx8yCSAM",
   "metadata": {
    "id": "q6ZuFx8yCSAM"
   },
   "source": [
    "This means that this would look back up to 721 hours (~30 days) for each time step, as additional features. However, the resulting feature vector would end up being of size `len(lags_sequence)*num_of_variates` which for our case will be 34480! This is not going to work so we will use our own sensible lags.\n",
    "\n",
    "Let us also check the default time features which GluonTS provides us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "VlP_0E5I76lg",
   "metadata": {
    "id": "VlP_0E5I76lg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function day_of_month at 0x164b0c040>, <function week_of_year at 0x164b0c400>]\n"
     ]
    }
   ],
   "source": [
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "\n",
    "time_features = time_features_from_frequency_str(freq)\n",
    "print(time_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m0_f7lm9CbNG",
   "metadata": {
    "id": "m0_f7lm9CbNG"
   },
   "source": [
    "In this case, there are four additional features, namely \"hour of day\", \"day of week\", \"day of month\" and \"day of year\". This means that for each time step, we'll add these features as a scalar values. For example, consider the timestamp `2015-01-01 01:00:01`. The four additional features will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "99d3d336",
   "metadata": {
    "id": "99d3d336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'day_of_month': array([-0.4]), 'week_of_year': array([-0.5])}\n"
     ]
    }
   ],
   "source": [
    "from pandas.core.arrays.period import period_array\n",
    "\n",
    "timestamp = pd.Period(\"2015-01-01 01:00:01\", freq=freq)\n",
    "timestamp_as_index = pd.PeriodIndex(data=period_array([timestamp]))\n",
    "additional_features = [\n",
    "    (time_feature.__name__, time_feature(timestamp_as_index))\n",
    "    for time_feature in time_features\n",
    "]\n",
    "print(dict(additional_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd7b8b9",
   "metadata": {
    "id": "9bd7b8b9"
   },
   "source": [
    "Note that hours and days are encoded as values between `[-0.5, 0.5]` from GluonTS. For more information about `time_features`, please see [this](https://github.com/awslabs/gluonts/blob/dev/src/gluonts/time_feature/_base.py). Besides those 4 features, we'll also add an \"age\" feature as we'll see later on in the data transformations.\n",
    "\n",
    "We now have everything to define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3dda0e78",
   "metadata": {
    "id": "3dda0e78"
   },
   "outputs": [],
   "source": [
    "from transformers import InformerConfig, InformerForPrediction\n",
    "\n",
    "config = InformerConfig(\n",
    "    # in the multivariate setting, input_size is the number of variates in the time series per time step\n",
    "    input_size=num_of_variates,\n",
    "    # prediction length:\n",
    "    prediction_length=prediction_length,\n",
    "    # context length:\n",
    "    context_length=prediction_length * 2,\n",
    "    # lags value copied from 1 week before:\n",
    "    lags_sequence=[1, 24 * 7],\n",
    "    # we'll add 5 time features (\"hour_of_day\", ..., and \"age\"):\n",
    "    num_time_features=len(time_features) + 1,\n",
    "    # informer params:\n",
    "    dropout=0.1,\n",
    "    encoder_layers=6,\n",
    "    decoder_layers=4,\n",
    "    # project input from num_of_variates*len(lags_sequence)+num_time_features to:\n",
    "    d_model=64,\n",
    ")\n",
    "\n",
    "model = InformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T6rCeV4dsUnF",
   "metadata": {
    "id": "T6rCeV4dsUnF"
   },
   "source": [
    "By default, the model uses a diagonal Student-t distribution (but this is [configurable](https://huggingface.co/docs/transformers/main/en/internal/time_series_utils)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "EaoKZyujsuIA",
   "metadata": {
    "id": "EaoKZyujsuIA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'student_t'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.distribution_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82c60d",
   "metadata": {
    "id": "af82c60d"
   },
   "source": [
    "## Define Transformations\n",
    "\n",
    "Next, we define the transformations for the data, in particular for the creation of the time features (based on the dataset or universal ones).\n",
    "\n",
    "Again, we'll use the GluonTS library for this. We define a `Chain` of transformations (which is a bit comparable to `torchvision.transforms.Compose` for images). It allows us to combine several transformations into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "tR87yKPSn8SZ",
   "metadata": {
    "id": "tR87yKPSn8SZ"
   },
   "outputs": [],
   "source": [
    "from gluonts.time_feature import TimeFeature\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    AsNumpyArray,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    RemoveFields,\n",
    "    SelectFields,\n",
    "    SetField,\n",
    "    TestSplitSampler,\n",
    "    Transformation,\n",
    "    ValidationSplitSampler,\n",
    "    VstackFeatures,\n",
    "    RenameFields,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4Qql4CInFWO7",
   "metadata": {
    "id": "4Qql4CInFWO7"
   },
   "source": [
    "The transformations below are annotated with comments, to explain what they do. At a high level, we will iterate over the individual time series of our dataset and add/remove fields or features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "20fe036e",
   "metadata": {
    "id": "20fe036e"
   },
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
    "    # create list of fields to remove later\n",
    "    remove_field_names = []\n",
    "    if config.num_static_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
    "    if config.num_dynamic_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
    "    if config.num_static_categorical_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
    "\n",
    "    return Chain(\n",
    "        # step 1: remove static/dynamic fields if not specified\n",
    "        [RemoveFields(field_names=remove_field_names)]\n",
    "        # step 2: convert the data to NumPy (potentially not needed)\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_CAT,\n",
    "                    expected_ndim=1,\n",
    "                    dtype=int,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_REAL,\n",
    "                    expected_ndim=1,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_real_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + [\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.TARGET,\n",
    "                # we expect an extra dim for the multivariate case:\n",
    "                expected_ndim=1 if config.input_size == 1 else 2,\n",
    "            ),\n",
    "            # step 3: handle the NaN's by filling in the target with zero\n",
    "            # and return the mask (which is in the observed values)\n",
    "            # true for observed values, false for nan's\n",
    "            # the decoder uses this mask (no loss is incurred for unobserved values)\n",
    "            # see loss_weights inside the xxxForPrediction model\n",
    "            AddObservedValuesIndicator(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.OBSERVED_VALUES,\n",
    "            ),\n",
    "            # step 4: add temporal features based on freq of the dataset\n",
    "            # these serve as positional encodings\n",
    "            AddTimeFeatures(\n",
    "                start_field=FieldName.START,\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                time_features=time_features_from_frequency_str(freq),\n",
    "                pred_length=config.prediction_length,\n",
    "            ),\n",
    "            # step 5: add another temporal feature (just a single number)\n",
    "            # tells the model where in the life the value of the time series is\n",
    "            # sort of running counter\n",
    "            AddAgeFeature(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_AGE,\n",
    "                pred_length=config.prediction_length,\n",
    "                log_scale=True,\n",
    "            ),\n",
    "            # step 6: vertically stack all the temporal features into the key FEAT_TIME\n",
    "            VstackFeatures(\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
    "                + (\n",
    "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
    "                    if config.num_dynamic_real_features > 0\n",
    "                    else []\n",
    "                ),\n",
    "            ),\n",
    "            # step 7: rename to match HuggingFace names\n",
    "            RenameFields(\n",
    "                mapping={\n",
    "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
    "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
    "                    FieldName.FEAT_TIME: \"time_features\",\n",
    "                    FieldName.TARGET: \"values\",\n",
    "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab46d0",
   "metadata": {
    "id": "8bab46d0"
   },
   "source": [
    "## Define `InstanceSplitter`\n",
    "\n",
    "For training/validation/testing we next create an `InstanceSplitter` which is used to sample windows from the dataset (as, remember, we can't pass the entire history of values to the model due to time- and memory constraints).\n",
    "\n",
    "The instance splitter samples random `context_length` sized and subsequent `prediction_length` sized windows from the data, and appends a `past_` or `future_` key to any temporal keys in `time_series_fields` for the respective windows. The instance splitter can be configured into three different modes:\n",
    "1. `mode=\"train\"`: Here we sample the context and prediction length windows randomly from the dataset given to it (the training dataset)\n",
    "2. `mode=\"validation\"`: Here we sample the very last context length window and prediction window from the dataset given to it (for the back-testing or validation likelihood calculations)\n",
    "3. `mode=\"test\"`: Here we sample the very last context length window only (for the prediction use case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cae7600d",
   "metadata": {
    "id": "cae7600d"
   },
   "outputs": [],
   "source": [
    "from gluonts.transform.sampler import InstanceSampler\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def create_instance_splitter(\n",
    "    config: PretrainedConfig,\n",
    "    mode: str,\n",
    "    train_sampler: Optional[InstanceSampler] = None,\n",
    "    validation_sampler: Optional[InstanceSampler] = None,\n",
    ") -> Transformation:\n",
    "    assert mode in [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    instance_sampler = {\n",
    "        \"train\": train_sampler\n",
    "        or ExpectedNumInstanceSampler(\n",
    "            num_instances=1.0, min_future=config.prediction_length\n",
    "        ),\n",
    "        \"validation\": validation_sampler\n",
    "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
    "        \"test\": TestSplitSampler(),\n",
    "    }[mode]\n",
    "\n",
    "    return InstanceSplitter(\n",
    "        target_field=\"values\",\n",
    "        is_pad_field=FieldName.IS_PAD,\n",
    "        start_field=FieldName.START,\n",
    "        forecast_start_field=FieldName.FORECAST_START,\n",
    "        instance_sampler=instance_sampler,\n",
    "        past_length=config.context_length + max(config.lags_sequence),\n",
    "        future_length=config.prediction_length,\n",
    "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958e050d",
   "metadata": {
    "id": "958e050d"
   },
   "source": [
    "## Create DataLoaders\n",
    "\n",
    "Next, it's time to create the DataLoaders, which allow us to have batches of (input, output) pairs - or in other words (`past_values`, `future_values`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6995101c",
   "metadata": {
    "id": "6995101c"
   },
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "from gluonts.itertools import Cached, Cyclic\n",
    "from gluonts.dataset.loader import as_stacked_batches\n",
    "\n",
    "\n",
    "def create_train_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    num_batches_per_epoch: int,\n",
    "    shuffle_buffer_length: Optional[int] = None,\n",
    "    cache_data: bool = True,\n",
    "    **kwargs,\n",
    ") -> Iterable:\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
    "        \"future_values\",\n",
    "        \"future_observed_mask\",\n",
    "    ]\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=True)\n",
    "    if cache_data:\n",
    "        transformed_data = Cached(transformed_data)\n",
    "\n",
    "    # we initialize a Training instance\n",
    "    instance_splitter = create_instance_splitter(config, \"train\")\n",
    "\n",
    "    # the instance splitter will sample a window of\n",
    "    # context length + lags + prediction length (from all the possible transformed time series, 1 in our case)\n",
    "    # randomly from within the target time series and return an iterator.\n",
    "    stream = Cyclic(transformed_data).stream()\n",
    "    training_instances = instance_splitter.apply(stream)\n",
    "\n",
    "    return as_stacked_batches(\n",
    "        training_instances,\n",
    "        batch_size=batch_size,\n",
    "        shuffle_buffer_length=shuffle_buffer_length,\n",
    "        field_names=TRAINING_INPUT_NAMES,\n",
    "        output_type=torch.tensor,\n",
    "        num_batches_per_epoch=num_batches_per_epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "10c55455",
   "metadata": {
    "id": "10c55455"
   },
   "outputs": [],
   "source": [
    "def create_backtest_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data)\n",
    "\n",
    "    # we create a Validation Instance splitter which will sample the very last\n",
    "    # context window seen during training only for the encoder.\n",
    "    instance_sampler = create_instance_splitter(config, \"validation\")\n",
    "\n",
    "    # we apply the transformations in train mode\n",
    "    testing_instances = instance_sampler.apply(transformed_data, is_train=True)\n",
    "\n",
    "    return as_stacked_batches(\n",
    "        testing_instances,\n",
    "        batch_size=batch_size,\n",
    "        output_type=torch.tensor,\n",
    "        field_names=PREDICTION_INPUT_NAMES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "997e43dd-3861-452b-98c5-b5ca3c7da4af",
   "metadata": {
    "id": "997e43dd-3861-452b-98c5-b5ca3c7da4af"
   },
   "outputs": [],
   "source": [
    "def create_test_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=False)\n",
    "\n",
    "    # We create a test Instance splitter to sample the very last\n",
    "    # context window from the dataset provided.\n",
    "    instance_sampler = create_instance_splitter(config, \"test\")\n",
    "\n",
    "    # We apply the transformations in test mode\n",
    "    testing_instances = instance_sampler.apply(transformed_data, is_train=False)\n",
    "\n",
    "    return as_stacked_batches(\n",
    "        testing_instances,\n",
    "        batch_size=batch_size,\n",
    "        output_type=torch.tensor,\n",
    "        field_names=PREDICTION_INPUT_NAMES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "20e2338b",
   "metadata": {
    "id": "20e2338b"
   },
   "outputs": [],
   "source": [
    "train_dataloader = create_train_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=multi_variate_train_dataset,\n",
    "    batch_size=256,\n",
    "    num_batches_per_epoch=100,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "test_dataloader = create_backtest_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=multi_variate_test_dataset,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ES2U8g-2G2Jd",
   "metadata": {
    "id": "ES2U8g-2G2Jd"
   },
   "source": [
    "Let's check the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "YU2h9OOB5IsX",
   "metadata": {
    "id": "YU2h9OOB5IsX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past_time_features torch.Size([256, 184, 3]) torch.FloatTensor\n",
      "past_values torch.Size([256, 184, 266]) torch.FloatTensor\n",
      "past_observed_mask torch.Size([256, 184, 266]) torch.FloatTensor\n",
      "future_time_features torch.Size([256, 8, 3]) torch.FloatTensor\n",
      "future_values torch.Size([256, 8, 266]) torch.FloatTensor\n",
      "future_observed_mask torch.Size([256, 8, 266]) torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape, v.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HvvPlSF8HBYd",
   "metadata": {
    "id": "HvvPlSF8HBYd"
   },
   "source": [
    "As can be seen, we don't feed `input_ids` and `attention_mask` to the encoder (as would be the case for NLP models), but rather `past_values`, along with `past_observed_mask`, `past_time_features` and `static_real_features`.\n",
    "\n",
    "The decoder inputs consist of `future_values`, `future_observed_mask` and `future_time_features`. The `future_values` can be seen as the equivalent of `decoder_input_ids` in NLP.\n",
    "\n",
    "We refer to the [docs](https://huggingface.co/docs/transformers/main/en/model_doc/informer#transformers.InformerForPrediction.forward.past_values) for a detailed explanation for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_cev4ufVv1yf",
   "metadata": {
    "id": "_cev4ufVv1yf"
   },
   "source": [
    "## Forward pass\n",
    "\n",
    "Let's perform a single forward pass with the batch we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "sD8fav6qTApR",
   "metadata": {
    "id": "sD8fav6qTApR"
   },
   "outputs": [],
   "source": [
    "# perform forward pass\n",
    "outputs = model(\n",
    "    past_values=batch[\"past_values\"],\n",
    "    past_time_features=batch[\"past_time_features\"],\n",
    "    past_observed_mask=batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=batch[\"static_categorical_features\"]\n",
    "    if config.num_static_categorical_features > 0\n",
    "    else None,\n",
    "    static_real_features=batch[\"static_real_features\"]\n",
    "    if config.num_static_real_features > 0\n",
    "    else None,\n",
    "    future_values=batch[\"future_values\"],\n",
    "    future_time_features=batch[\"future_time_features\"],\n",
    "    future_observed_mask=batch[\"future_observed_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "j2KnnHTCX4RC",
   "metadata": {
    "id": "j2KnnHTCX4RC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -750.0942993164062\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss:\", outputs.loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V9K8s2j9y8x7",
   "metadata": {
    "id": "V9K8s2j9y8x7"
   },
   "source": [
    "Note that the model is returning a loss. This is possible as the decoder automatically shifts the `future_values` one position to the right in order to have the labels. This allows computing a loss between the predicted values and the labels. The loss is the negative log-likelihood of the predicted distribution with respect to the ground truth values and tends to negative infinity.\n",
    "\n",
    "Also note that the decoder uses a causal mask to not look into the future as the values it needs to predict are in the `future_values` tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SxHDCa7vwPBF",
   "metadata": {
    "id": "SxHDCa7vwPBF"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "It's time to train the model! We'll use a standard PyTorch training loop.\n",
    "\n",
    "We will use the ðŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index) library here, which automatically places the model, optimizer and dataloader on the appropriate `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gMLYvQaNHuXQ",
   "metadata": {
    "id": "gMLYvQaNHuXQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 & idx 0 with loss: -525.2201538085938\n",
      "Epoch 1 & idx 0 with loss: -739.9453125\n",
      "Epoch 2 & idx 0 with loss: -742.2052001953125\n",
      "Epoch 3 & idx 0 with loss: -791.9387817382812\n",
      "Epoch 4 & idx 0 with loss: -872.11572265625\n",
      "Epoch 5 & idx 0 with loss: -845.2001953125\n",
      "Epoch 6 & idx 0 with loss: -1078.78125\n",
      "Epoch 7 & idx 0 with loss: -1018.3587036132812\n",
      "Epoch 8 & idx 0 with loss: -881.9295654296875\n",
      "Epoch 9 & idx 0 with loss: -986.2193603515625\n",
      "Epoch 10 & idx 0 with loss: -977.1693115234375\n",
      "Epoch 11 & idx 0 with loss: -1146.959716796875\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "\n",
    "epochs = 50\n",
    "loss_history = []\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=6e-5, betas=(0.9, 0.95), weight_decay=1e-1)\n",
    "\n",
    "model, optimizer, train_dataloader = accelerator.prepare(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else None,\n",
    "            static_real_features=batch[\"static_real_features\"].to(device)\n",
    "            if config.num_static_real_features > 0\n",
    "            else None,\n",
    "            past_time_features=batch[\"past_time_features\"].to(device),\n",
    "            past_values=batch[\"past_values\"].to(device),\n",
    "            future_time_features=batch[\"future_time_features\"].to(device),\n",
    "            future_values=batch[\"future_values\"].to(device),\n",
    "            past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "            future_observed_mask=batch[\"future_observed_mask\"].to(device),\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        if idx % 200 == 0:\n",
    "            print(f'Epoch {epoch} & idx {idx} with loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed98244-bf84-4efe-ba57-7519c96ece61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "work_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e9bb8",
   "metadata": {
    "id": "cb1e9bb8"
   },
   "outputs": [],
   "source": [
    "# view training\n",
    "from datetime import datetime\n",
    "\n",
    "loss_history = np.array(loss_history).reshape(-1)\n",
    "x = range(loss_history.shape[0])\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, loss_history, label=\"train\")\n",
    "plt.title(\"Loss\", fontsize=15)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"nll\")\n",
    "plt.savefig(data_dir+\"hf_informer4roqeto_loss_curve_\"+str(datetime.now()).replace(\":\",\".\")+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q1U6YhaFXlSp",
   "metadata": {
    "id": "Q1U6YhaFXlSp"
   },
   "source": [
    "## Inference\n",
    "\n",
    "At inference time, it's recommended to use the `generate()` method for autoregressive generation, similar to NLP models.\n",
    "\n",
    "Forecasting involves getting data from the test instance sampler, which will sample the very last `context_length` sized window of values from each time series in the dataset, and pass it to the model. Note that we pass `future_time_features`, which are known ahead of time, to the decoder.\n",
    "\n",
    "The model will autoregressively sample a certain number of values from the predicted distribution and pass them back to the decoder to return the prediction outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68210526-6ff3-475d-986b-a3e188ad47ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c7482c1",
   "metadata": {
    "id": "7c7482c1"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_standard_gamma' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m forecasts_ = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstatic_categorical_features\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_static_categorical_features\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstatic_real_features\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_static_real_features\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpast_time_features\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpast_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfuture_time_features\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpast_observed_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     forecasts_.append(outputs.sequences.cpu().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/transformers/models/informer/modeling_informer.py:2049\u001b[39m, in \u001b[36mInformerForPrediction.generate\u001b[39m\u001b[34m(self, past_values, past_time_features, future_time_features, past_observed_mask, static_categorical_features, static_real_features, output_attentions, output_hidden_states)\u001b[39m\n\u001b[32m   2047\u001b[39m params = \u001b[38;5;28mself\u001b[39m.parameter_projection(dec_last_hidden[:, -\u001b[32m1\u001b[39m:])\n\u001b[32m   2048\u001b[39m distr = \u001b[38;5;28mself\u001b[39m.output_distribution(params, loc=repeated_loc, scale=repeated_scale)\n\u001b[32m-> \u001b[39m\u001b[32m2049\u001b[39m next_sample = \u001b[43mdistr\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2051\u001b[39m repeated_past_values = torch.cat(\n\u001b[32m   2052\u001b[39m     (repeated_past_values, (next_sample - repeated_loc) / repeated_scale), dim=\u001b[32m1\u001b[39m\n\u001b[32m   2053\u001b[39m )\n\u001b[32m   2054\u001b[39m future_samples.append(next_sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:141\u001b[39m, in \u001b[36mTransformedDistribution.sample\u001b[39m\u001b[34m(self, sample_shape)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03mGenerates a sample_shape shaped sample or sample_shape shaped batch of\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03msamples if the distribution parameters are batched. Samples first from\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[33;03mbase distribution and applies `transform()` for every transform in the\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03mlist.\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m    143\u001b[39m         x = transform(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/torch/distributions/independent.py:104\u001b[39m, in \u001b[36mIndependent.sample\u001b[39m\u001b[34m(self, sample_shape)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample_shape=torch.Size()):\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/torch/distributions/distribution.py:167\u001b[39m, in \u001b[36mDistribution.sample\u001b[39m\u001b[34m(self, sample_shape)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03mGenerates a sample_shape shaped sample or sample_shape shaped batch of\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03msamples if the distribution parameters are batched.\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/torch/distributions/studentT.py:89\u001b[39m, in \u001b[36mStudentT.rsample\u001b[39m\u001b[34m(self, sample_shape)\u001b[39m\n\u001b[32m     87\u001b[39m shape = \u001b[38;5;28mself\u001b[39m._extended_shape(sample_shape)\n\u001b[32m     88\u001b[39m X = _standard_normal(shape, dtype=\u001b[38;5;28mself\u001b[39m.df.dtype, device=\u001b[38;5;28mself\u001b[39m.df.device)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m Z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chi2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m Y = X * torch.rsqrt(Z / \u001b[38;5;28mself\u001b[39m.df)\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loc + \u001b[38;5;28mself\u001b[39m.scale * Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/torch/distributions/gamma.py:74\u001b[39m, in \u001b[36mGamma.rsample\u001b[39m\u001b[34m(self, sample_shape)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrsample\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample_shape: _size = torch.Size()) -> torch.Tensor:\n\u001b[32m     73\u001b[39m     shape = \u001b[38;5;28mself\u001b[39m._extended_shape(sample_shape)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     value = \u001b[43m_standard_gamma\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconcentration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m / \u001b[38;5;28mself\u001b[39m.rate.expand(\n\u001b[32m     75\u001b[39m         shape\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     77\u001b[39m     value.detach().clamp_(\n\u001b[32m     78\u001b[39m         \u001b[38;5;28mmin\u001b[39m=torch.finfo(value.dtype).tiny\n\u001b[32m     79\u001b[39m     )  \u001b[38;5;66;03m# do not record in autograd graph\u001b[39;00m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gpu_py311/lib/python3.11/site-packages/torch/distributions/gamma.py:15\u001b[39m, in \u001b[36m_standard_gamma\u001b[39m\u001b[34m(concentration)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_standard_gamma\u001b[39m(concentration):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_standard_gamma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcentration\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mNotImplementedError\u001b[39m: The operator 'aten::_standard_gamma' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "forecasts_ = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    outputs = model.generate(\n",
    "        static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "        if config.num_static_categorical_features > 0\n",
    "        else None,\n",
    "        static_real_features=batch[\"static_real_features\"].to(device)\n",
    "        if config.num_static_real_features > 0\n",
    "        else None,\n",
    "        past_time_features=batch[\"past_time_features\"].to(device),\n",
    "        past_values=batch[\"past_values\"].to(device),\n",
    "        future_time_features=batch[\"future_time_features\"].to(device),\n",
    "        past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "    )\n",
    "    forecasts_.append(outputs.sequences.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kPLiRcOeZR67",
   "metadata": {
    "id": "kPLiRcOeZR67"
   },
   "source": [
    "The model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `input_size`).\n",
    "\n",
    "In this case, we get `100` possible values for the next `48` hours for each of the `862` time series (for each example in the batch which is of size `1` since we only have a single multivariate time series):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DwAfSZitZNAQ",
   "metadata": {
    "id": "DwAfSZitZNAQ"
   },
   "outputs": [],
   "source": [
    "forecasts_[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fCTBw_t91xwH",
   "metadata": {
    "id": "fCTBw_t91xwH"
   },
   "source": [
    "We'll stack them vertically, to get forecasts for all time-series in the test dataset (just in case there are more time series in the test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "du1GyJVXlpHp",
   "metadata": {
    "id": "du1GyJVXlpHp"
   },
   "outputs": [],
   "source": [
    "forecasts = np.vstack(forecasts_)\n",
    "print(forecasts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlvFCdgiA9oy",
   "metadata": {
    "id": "wlvFCdgiA9oy"
   },
   "source": [
    "We can evaluate the resulting forecast with respect to the ground truth out of sample values present in the test set. For that, we'll use the ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) library, which includes the [MASE](https://huggingface.co/spaces/evaluate-metric/mase) and [sMAPE](https://huggingface.co/spaces/evaluate-metric/smape) metrics.\n",
    "\n",
    "We calculate both metrics for each time series variate in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0yb9RnczYE4z",
   "metadata": {
    "id": "0yb9RnczYE4z"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "mase_metric = load(\"evaluate-metric/mase\")\n",
    "smape_metric = load(\"evaluate-metric/smape\")\n",
    "\n",
    "forecast_median = np.median(forecasts, 1).squeeze(0).T\n",
    "\n",
    "mase_metrics = []\n",
    "smape_metrics = []\n",
    "\n",
    "for item_id, ts in enumerate(test_dataset):\n",
    "    training_data = ts[\"target\"][:-prediction_length]\n",
    "    ground_truth = ts[\"target\"][-prediction_length:]\n",
    "    mase = mase_metric.compute(\n",
    "        predictions=forecast_median[item_id],\n",
    "        references=np.array(ground_truth),\n",
    "        training=np.array(training_data),\n",
    "        periodicity=get_seasonality(freq),\n",
    "    )\n",
    "    mase_metrics.append(mase[\"mase\"])\n",
    "\n",
    "    smape = smape_metric.compute(\n",
    "        predictions=forecast_median[item_id],\n",
    "        references=np.array(ground_truth),\n",
    "    )\n",
    "    smape_metrics.append(smape[\"smape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuuFbNYdZlIR",
   "metadata": {
    "id": "fuuFbNYdZlIR"
   },
   "outputs": [],
   "source": [
    "print(f\"MASE: {np.mean(mase_metrics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w6ETpUrML2wE",
   "metadata": {
    "id": "w6ETpUrML2wE"
   },
   "outputs": [],
   "source": [
    "print(f\"sMAPE: {np.mean(smape_metrics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b649b65e",
   "metadata": {
    "id": "b649b65e"
   },
   "outputs": [],
   "source": [
    "plt.scatter(mase_metrics, smape_metrics, alpha=0.2)\n",
    "plt.xlabel(\"MASE\")\n",
    "plt.ylabel(\"sMAPE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moksM2QmMACr",
   "metadata": {
    "id": "moksM2QmMACr"
   },
   "source": [
    "To plot the prediction for any time series variate with respect the ground truth test data we define the following helper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35b93e",
   "metadata": {
    "id": "4e35b93e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "def plot(ts_index, mv_index):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    index = pd.period_range(\n",
    "        start=multi_variate_test_dataset[ts_index][FieldName.START],\n",
    "        periods=len(multi_variate_test_dataset[ts_index][FieldName.TARGET]),\n",
    "        freq=multi_variate_test_dataset[ts_index][FieldName.START].freq,\n",
    "    ).to_timestamp()\n",
    "\n",
    "    ax.xaxis.set_minor_locator(mdates.HourLocator())\n",
    "\n",
    "    ax.plot(\n",
    "        index[-2 * prediction_length :],\n",
    "        multi_variate_test_dataset[ts_index][\"target\"][\n",
    "            mv_index, -2 * prediction_length :\n",
    "        ],\n",
    "        label=\"actual\",\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        index[-prediction_length:],\n",
    "        forecasts[ts_index, ..., mv_index].mean(axis=0),\n",
    "        label=\"mean\",\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        index[-prediction_length:],\n",
    "        forecasts[ts_index, ..., mv_index].mean(0)\n",
    "        - forecasts[ts_index, ..., mv_index].std(axis=0),\n",
    "        forecasts[ts_index, ..., mv_index].mean(0)\n",
    "        + forecasts[ts_index, ..., mv_index].std(axis=0),\n",
    "        alpha=0.2,\n",
    "        interpolate=True,\n",
    "        label=\"+/- 1-std\",\n",
    "    )\n",
    "    ax.legend()\n",
    "    fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mwtksAcxMHoK",
   "metadata": {
    "id": "mwtksAcxMHoK"
   },
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5N8fdnm_MKQP",
   "metadata": {
    "id": "5N8fdnm_MKQP"
   },
   "outputs": [],
   "source": [
    "plot(0, 344)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nf4Y8MvLMOi8",
   "metadata": {
    "id": "Nf4Y8MvLMOi8"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "How do we compare against other models? The [Monash Time Series Repository](https://forecastingdata.org/#results) has a comparison table of test set MASE metrics which we can add to:\n",
    "\n",
    "|Dataset | \tSES| \tTheta | \tTBATS| \tETS\t| (DHR-)ARIMA| \tPR|\tCatBoost |\tFFNN\t| DeepAR | \tN-BEATS | \tWaveNet|  Transformer (uni.) | **Informer (mv. our)**|\n",
    "|:------------------:|:-----------------:|:--:|:--:|:--:|:--:|:--:|:--:|:---:|:---:|:--:|:--:|:--:|:--:|\n",
    "|Traffic Hourly | 1.922\t| 1.922\t| 2.482 |\t2.294|\t2.535|\t1.281|\t1.571\t|0.892|\t0.825\t|1.100|\t1.066\t| **0.821** | 1.191 |\n",
    "\n",
    "As can be seen, and perhaps surprising to some, the multivariate forecasts are typically _worse_ than the univariate ones, the reason being the difficulty in estimating the cross-series correlations/relationships. The additional variance added by the estimates often harms the resulting forecasts or the model learns spurious correlations. We refer to [this paper](https://openreview.net/forum?id=GpW327gxLTF) for further reading. Multivariate models tend to work well when trained on a lot of data.\n",
    "\n",
    "So the vanilla Transformer still performs best here! In the future, we hope to better benchmark these models in a central place to ease reproducing the results of several papers. Stay tuned for more!\n",
    "\n",
    "## Resources\n",
    "\n",
    "We recommend to check out the [Informer docs](https://huggingface.co/docs/transformers/main/en/model_doc/informer) and the example notebook linked at the top of this blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04e604",
   "metadata": {
    "id": "ec04e604"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
